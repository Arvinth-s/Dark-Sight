{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskRCNN_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOM4Sbqbkv770heG+u6eFBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arvinth-s/Dark-Sight/blob/master/MaskRCNN_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQyrT_Gl1eW"
      },
      "source": [
        "#OM NAMO NARAYANA"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogLRhyNXmDu2"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLxTGnQLm45w",
        "outputId": "79dc4032-601c-4693-c0b1-6cdbda9c18ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#We use '!' for running shell command\n",
        "#Implemented on the base of pytorch tutorial\n",
        "#https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
        "if ('PennFudanPed.zip' not in os.listdir('/content/')):\n",
        "  !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "else:\n",
        "  print('Already downloaded')\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkGOKMEnUz_",
        "outputId": "617bf571-09bf-47f7-d1bd-1663e34531c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if(\"PennFudanPed\" not in os.listdir('/content/')):\n",
        "  !unzip 'PennFudanPed.zip'\n",
        "else:\n",
        "  print('Already unzipped')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already unzipped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP_ydo2-o4iT",
        "outputId": "093a5352-2d2a-4311-c9f9-3be035704d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.chdir('/content/PennFudanPed')\n",
        "os.listdir('./')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Annotation', 'PNGImages', 'readme.txt', 'PedMasks', 'added-object-list.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNFaKbvp1sob"
      },
      "source": [
        "#__setitem__ and __getitem__\n",
        "#https://stackoverflow.com/questions/43627405/understanding-getitem-method#:~:text=The%20magic%20method%20__getitem,'%20(date%20of%20birth).\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        # img.show()\n",
        "        # plt.imshow(img)\n",
        "        # print(img)\n",
        "        # print('showing image')\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        # print('my target', target)\n",
        "        return (img, target)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-tS-JfF6MLM"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCsKHs3Cs1IR"
      },
      "source": [
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5AV_y9tNMi"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRnB8BHItQxs"
      },
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    print(type(train))\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    # if train:\n",
        "    #     transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkjmBwHdnxbt"
      },
      "source": [
        "def my_collate(batch):\n",
        "  return ([cbatch[0] for cbatch in batch], [cbatch[1] for cbatch in batch])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCY_6CNXlKwN",
        "outputId": "be3de364-7310-43a0-8208-700858226864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "import utils\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = PennFudanDataset('/content/PennFudanPed', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " dataset, batch_size=1, shuffle=True, num_workers=4,\n",
        " collate_fn=my_collate)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "print('images', images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "print('target', targets)\n",
        "\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'bool'>\n",
            "images [tensor([[[0.2000, 0.2000, 0.1882,  ..., 0.3255, 0.2941, 0.2588],\n",
            "         [0.1922, 0.1922, 0.1843,  ..., 0.5294, 0.5176, 0.5020],\n",
            "         [0.1922, 0.1961, 0.1922,  ..., 0.3647, 0.3843, 0.4039],\n",
            "         ...,\n",
            "         [0.4627, 0.4667, 0.4706,  ..., 0.6039, 0.5961, 0.5961],\n",
            "         [0.4588, 0.4706, 0.4784,  ..., 0.6078, 0.6000, 0.6000],\n",
            "         [0.4588, 0.4784, 0.4863,  ..., 0.6078, 0.6039, 0.6039]],\n",
            "\n",
            "        [[0.2745, 0.2745, 0.2745,  ..., 0.3333, 0.3020, 0.2667],\n",
            "         [0.2667, 0.2667, 0.2706,  ..., 0.5373, 0.5255, 0.5098],\n",
            "         [0.2706, 0.2745, 0.2784,  ..., 0.3725, 0.3922, 0.4118],\n",
            "         ...,\n",
            "         [0.4863, 0.4902, 0.4941,  ..., 0.6118, 0.6039, 0.6039],\n",
            "         [0.4824, 0.4941, 0.5020,  ..., 0.6157, 0.6078, 0.6078],\n",
            "         [0.4824, 0.5020, 0.5098,  ..., 0.6157, 0.6118, 0.6118]],\n",
            "\n",
            "        [[0.3294, 0.3294, 0.3255,  ..., 0.3137, 0.2824, 0.2471],\n",
            "         [0.3216, 0.3216, 0.3216,  ..., 0.5176, 0.5059, 0.4902],\n",
            "         [0.3137, 0.3176, 0.3294,  ..., 0.3529, 0.3725, 0.3922],\n",
            "         ...,\n",
            "         [0.4706, 0.4745, 0.4863,  ..., 0.6078, 0.6000, 0.6000],\n",
            "         [0.4667, 0.4784, 0.4941,  ..., 0.6118, 0.6039, 0.6039],\n",
            "         [0.4667, 0.4863, 0.5020,  ..., 0.6118, 0.6078, 0.6078]]])]\n",
            "target [{'boxes': tensor([[227., 157., 369., 435.],\n",
            "        [ 38., 178., 114., 362.]]), 'labels': tensor([1, 1]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), 'image_id': tensor([7]), 'area': tensor([39476., 13984.]), 'iscrowd': tensor([0, 0])}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U8WRBjC9N_I"
      },
      "source": [
        "def train_model(model, optimizer, scheduler, dataloader, num_epochs):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for phase in ['train', 'eval']:\n",
        "      if(phase == 'train'):\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "      running_loss = 0.0\n",
        "      runnin_corrects = 0\n",
        "\n",
        "      for inputs, labels in iter(data_loader):\n",
        "        # inputs = inputs.to(device)\n",
        "        # labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # running_loss = 0\n",
        "\n",
        "        with torch.set_grad_enabled(phase=='train'):\n",
        "          # outputs=model(inputs)\n",
        "          # _, preds = torch.max(outputs, 1)\n",
        "          loss = model(inputs, labels)\n",
        "\n",
        "          if(phase == 'train'):\n",
        "            for item in loss:\n",
        "              print(loss[item])\n",
        "              # running_loss = loss[item].numpy()\n",
        "              loss[item].backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            # scheduler.step()\n",
        "\n",
        "          # running_loss += loss.item() * input.size(0)\n",
        "          \n",
        "          # running_corrects += torch.sum(preds==labels.data)\n",
        "      if(phase=='train'):\n",
        "        scheduler.step()\n",
        "      # epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      # epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "      # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "          # phase, epoch_loss, epoch_acc\n",
        "      # ))\n",
        "      return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmZUeDrcttaY",
        "outputId": "d70aa38f-a6a6-4f8a-e9ca-1173599ef0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "def main():\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    num_classes = 2\n",
        "    dataset = PennFudanDataset('/content/PennFudanPed', get_transform(train=True))\n",
        "    dataset_test = PennFudanDataset('/content/PennFudanPed', get_transform(train=False))\n",
        "\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=2, shuffle=True, num_workers=4,)\n",
        "        # collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=1, shuffle=False, num_workers=4,)\n",
        "        # collate_fn=utils.collate_fn)\n",
        "\n",
        "    # get the model using our helper function\n",
        "    model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=3,\n",
        "                                                   gamma=0.1)\n",
        "\n",
        "    # let's train it for 10 epochs\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # # train for one epoch, printing every 10 iterations\n",
        "        # train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        # # model.()\n",
        "        # # update the learning rate\n",
        "        # lr_scheduler.step()\n",
        "        # # evaluate on the test dataset\n",
        "        # evaluate(model, data_loader_test, device=device)\n",
        "        # # model.eval()\n",
        "        train_model(model, optimizer, lr_scheduler, data_loader, 10)\n",
        "        model = model.train()\n",
        "\n",
        "    print(\"That's it!\")\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'bool'>\n",
            "<class 'bool'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6910, grad_fn=<DivBackward0>)\n",
            "tensor(8.6035, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0224, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0096, grad_fn=<DivBackward0>)\n",
            "tensor(0.3849, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2535, grad_fn=<DivBackward0>)\n",
            "tensor(0.8486, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0071, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0037, grad_fn=<DivBackward0>)\n",
            "tensor(0.2174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1861, grad_fn=<DivBackward0>)\n",
            "tensor(1.6876, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0746, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0091, grad_fn=<DivBackward0>)\n",
            "tensor(0.0979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0907, grad_fn=<DivBackward0>)\n",
            "tensor(0.5116, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.1042, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0005, grad_fn=<DivBackward0>)\n",
            "tensor(0.2092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2541, grad_fn=<DivBackward0>)\n",
            "tensor(0.5166, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0464, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0100, grad_fn=<DivBackward0>)\n",
            "tensor(0.1777, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2193, grad_fn=<DivBackward0>)\n",
            "tensor(0.4936, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0221, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0079, grad_fn=<DivBackward0>)\n",
            "tensor(0.2321, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2873, grad_fn=<DivBackward0>)\n",
            "tensor(0.5731, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0139, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0099, grad_fn=<DivBackward0>)\n",
            "tensor(0.1126, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1344, grad_fn=<DivBackward0>)\n",
            "tensor(0.5373, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0139, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0049, grad_fn=<DivBackward0>)\n",
            "tensor(0.0349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0335, grad_fn=<DivBackward0>)\n",
            "tensor(1.0124, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0205, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0013, grad_fn=<DivBackward0>)\n",
            "tensor(0.0623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0939, grad_fn=<DivBackward0>)\n",
            "tensor(1.0600, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnHCqPL44qlc"
      },
      "source": [
        "# ! echo $PYTHONPATH\n",
        "\n",
        "# import os\n",
        "# print(os.environ['PYTHONPATH'])\n",
        "# os.environ['PYTHONPATH'] += \":/content/gdrive/My Drive/Colab Notebooks/MaskRCNN_tutorial/src\"\n",
        "# print(os.environ['PYTHONPATH'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyRDAp8U4r8W"
      },
      "source": [
        "# !pip install utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kilctl--7Eyd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eH7O1gq33Q"
      },
      "source": [
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAoSwZ4C0JEY"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Tn-KAs1X8q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}